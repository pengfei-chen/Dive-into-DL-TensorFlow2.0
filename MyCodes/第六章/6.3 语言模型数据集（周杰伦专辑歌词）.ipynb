{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "rural-transcription",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "source": [
    "本节将介绍如何预处理一个语言模型数据集，并将其转换成**字符级循环神经网络**所需要的输入格式。为此，我们收集了周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》中的歌词，并在后面几节里应用循环神经网络来训练一个语言模型。当模型训练好后，我们就可以用这个模型来**创作歌词**。"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "imported-filing",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "source": [
    "## 6.3.1 读取数据集\n",
    "\n",
    "首先读取这个数据集，看看前40个字符是什么样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
<<<<<<< HEAD
   "id": "unsigned-duncan",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('../../data/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "extensive-weekend",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "source": [
    "这个数据集有6万多个字符。为了打印方便，我们把换行符替换成空格，然后仅使用前1万个字符来训练模型。"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "id": "latest-verse",
=======
   "execution_count": 2,
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "restricted-shower",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "source": [
    "## 6.3.2 建立字符索引\n",
    "\n",
    "我们将每个字符映射成一个从0开始的连续整数，又称索引，来方便之后的数据处理。为了得到索引，我们**将数据集里所有不同字符取出来**，然后将其逐一映射到索引来构造词典。接着，打印`vocab_size`，即词典中不同字符的个数，又称词典大小。"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "id": "choice-nudist",
=======
   "execution_count": 3,
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char,i) for i,char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "backed-terrorism",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "source": [
    "之后，将训练数据集中每个字符转化为索引，并打印前20个字符及其对应的索引。"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "id": "preliminary-operation",
=======
   "execution_count": 4,
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [276, 235, 705, 866, 640, 977, 284, 276, 235, 579, 536, 874, 194, 432, 615, 753, 284, 276, 235, 579]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))  # 根据索引找到，其原来对应的 字符串\n",
    "print('indices:', sample) "
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "random-invention",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "source": [
    "**很好的想法。**"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "optional-committee",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "source": [
    "我们将以上代码封装在`d2lzh_tensorflow2`包里的`load_data_jay_lyrics`函数中，以方便后面章节调用。调用该函数后会依次得到`corpus_indices`、`char_to_idx`、`idx_to_char`和`vocab_size`这4个变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3 时序数据的采样\n",
    "\n",
    "在训练中我们需要**每次随机读取**小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。**该样本的标签序列为这些字符分别在训练集中的下一个字符**，即“要”“有”“直”“升”“机”。我们有两种方式对时序数据进行采样，分别是**随机采样**和**相邻采样**。\n",
    "\n",
    "### 6.3.3.1 随机采样\n",
    "\n",
    "下面的代码**每次从数据里随机采样一个小批量**。其中批量大小`batch_size`指每个小批量的样本数，`num_steps`为每个样本所包含的时间步数。\n",
    "在随机采样中，每个样本是原始序列上任意截取的一段序列。**相邻的两个随机小批量在原始序列上的位置不一定相毗邻**。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。**在训练模型时，每次随机采样前都需要重新初始化隐藏状态**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_tensorflow2包中方便以后使用\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    # 减 1 是因为输出的索引是相应输入的索引加 1\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))  # 比如一共有 500 个 examples,每一个都有一个索引\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # 返回从pos开始的长为num_steps的序列\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos:pos + num_steps]\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        #  每次读取batch_size 个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i:i + batch_size]\n",
    "        X = [_data(j * num_steps) for j in batch_indices]\n",
    "        Y = [_data(j * num_steps + 1) for j in batch_indices]   # 这里错开一位是为了？\n",
    "        yield np.array(X, ctx), np.array(Y, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们输入一个从0到29的连续整数的人工序列。设批量大小和时间步数分别为2和6。打印随机采样每次读取的小批量样本的输入`X`和标签`Y`。可见，**相邻的两个随机小批量在原始序列上的位置不一定相毗邻**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[ 6  7  8  9 10 11]\n",
      " [18 19 20 21 22 23]] \n",
      "Y:  [[ 7  8  9 10 11 12]\n",
      " [19 20 21 22 23 24]] \n",
      "\n",
      "X:  [[ 0  1  2  3  4  5]\n",
      " [12 13 14 15 16 17]] \n",
      "Y:  [[ 1  2  3  4  5  6]\n",
      " [13 14 15 16 17 18]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "my_seq = list(range(30))\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X,'\\nY: ', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**不一定相邻，指的是两块中的，X 的 索引 11 到 18 不相邻？  是指从11到0不相邻**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3.2 相邻采样\n",
    "\n",
    "除对原始序列做随机采样之外，我们还可以**令相邻的两个随机小批量在原始序列上的位置相毗邻**。这时候，我们就**可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态**，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：一方面，\n",
    "在训练模型时，**我们只需在每一个迭代周期开始时初始化隐藏状态**；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，**模型参数的梯度计算将依赖所有串联起来的小批量序列**。**同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大**。\n",
    "为了**使模型参数的梯度计算只依赖一次迭代读取的小批量序列**，我们可以**在每次读取小批量前将隐藏状态从计算图中分离出来**。我们将在下一节（循环神经网络的从零开始实现）的实现中了解这种处理方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_tensorflow2包中方便以后使用\n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = np.array(corpus_indices)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0:batch_size*batch_len].reshape((batch_size,batch_len))  # 这个例子中是 （2，15）\n",
    "    epoch_size = (batch_len - 1) //num_steps  # 15 -1 / 6 = 2 \n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps  # 第一个i = 0, 第二个 i = 6\n",
    "        X = indices[:, i:i + num_steps]\n",
    "        Y = indices[:, i + 1:i + num_steps + 1 ]\n",
    "        yield X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的设置下，打印相邻采样每次读取的小批量样本的输入`X`和标签`Y`。**相邻的两个随机小批量在原始序列上的位置相毗邻**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[ 0  1  2  3  4  5]\n",
      " [15 16 17 18 19 20]] \n",
      "Y: [[ 1  2  3  4  5  6]\n",
      " [16 17 18 19 20 21]] \n",
      "\n",
      "X:  [[ 6  7  8  9 10 11]\n",
      " [21 22 23 24 25 26]] \n",
      "Y: [[ 7  8  9 10 11 12]\n",
      " [22 23 24 25 26 27]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 时序数据采样方式包括**随机采样**和**相邻采样**。使用这两种方式的循环神经网络训练在实现上略有不同。\n",
    "\n",
    "-----------\n",
    "\n",
    "> 注：除代码外本节与原书此节基本相同，[原书传送门](https://zh.d2l.ai/chapter_recurrent-neural-networks/lang-model-dataset.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "closed-station",
=======
>>>>>>> a49da55c55327a9c75d7e2fbf6d3c509ffdc7fa4
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
