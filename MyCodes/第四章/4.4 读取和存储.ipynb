{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们介绍了如何处理数据以及如何构建、训练和测试深度学习模型。然而在实际中，我们有时需要把训练好的模型部署到很多不同的设备。在这种情况下，我们可以**把内存中训练好的模型参数存储在硬盘上**供后续读取使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.1 load and save NDarray  保存NDarray\n",
    "\n",
    "我们可以直接使用`save`函数和`load`函数分别**存储**和**读取**。下面的例子创建了tensor`x`，并将其存在文件名同为`x`的文件里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = tf.ones(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们将数据从存储的文件读回内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('x.npy', x)\n",
    "x2 = np.load('x.npy')   # npy文件，原来是这样来的\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以存储一列`tensor`并读回内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.zeros(4)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('xy.npy',[x,y])   # 这里的格式，需要学习下\n",
    "x2, y2 = np.load('xy.npy', allow_pickle=True)\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们甚至可以存储并读取一个从字符串映射到`tensor`的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'x': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>, 'y': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x':x, 'y':y}\n",
    "np.save('mydict.npy', mydict)\n",
    "mydict2 = np.load('mydict.npy', allow_pickle=True)\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 load and save model parameters  保存模型参数\n",
    "\n",
    "我们还可以读写模型的参数。\n",
    "为了演示方便，我们先创建一个多层感知机，并将其初始化。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
       "array([[-7.3711472e-03, -2.2265685e+00,  1.6618804e+00,  2.5140259e-01,\n",
       "        -7.2410059e-01, -1.0122397e+00,  2.5616276e-01, -4.1212311e-01,\n",
       "         4.8814896e-01,  9.1878396e-01, -2.7675667e-01, -1.0136944e+00,\n",
       "         1.2225481e+00,  6.1943024e-01,  2.0574312e-01,  9.0819165e-02,\n",
       "         3.4139338e-01, -1.3885272e-01,  1.4656541e+00,  4.3042353e-01],\n",
       "       [ 5.9879646e-03, -3.8827896e-01,  3.9519036e-01,  5.6982005e-01,\n",
       "         7.1759450e-01, -8.6360025e-01, -6.9179755e-01, -2.3116879e+00,\n",
       "         1.0306482e+00, -4.5546532e-02,  2.1602666e-01,  7.3800533e-04,\n",
       "         1.9990959e+00,  1.8492056e+00,  1.6333201e+00,  2.9170945e+00,\n",
       "        -1.6078517e+00, -1.0648005e+00, -8.0883843e-01,  2.1114954e-01]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal((2,20))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.28522736, -0.41203678,  0.22189781,  0.15751928, -0.22163066,\n",
       "        -0.16435435, -0.33152458, -0.08982636, -0.339939  , -0.1964671 ],\n",
       "       [ 0.793442  , -0.57389915, -0.75346386,  0.11127923, -0.15200633,\n",
       "        -0.2400654 , -0.74082786, -0.01516237, -0.17271438,  0.25783202]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         \n",
    "        x = self.flatten(inputs)   \n",
    "        x = self.dense1(x)    \n",
    "        output = self.dense2(x)     \n",
    "        return output\n",
    "\n",
    "net = MLP()\n",
    "Y = net(x)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面把该模型的参数存成文件，文件名为`4.5saved_model.h5`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_weights(\"4.5saved_model.h5\")   # 记住这里的表述方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们再实例化一次定义好的多层感知机。与随机初始化模型参数不同，我们在这里直接读取保存在文件里的参数。\n",
    "\n",
    "因为这两个实例都有同样的模型参数，那么对同一个输入X的计算结果将会是一样的。我们来验证一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = MLP()\n",
    "net2(x)\n",
    "net2.load_weights(\"4.5saved_model.h5\")\n",
    "Y2 = net2(x)\n",
    "Y2 == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注：本节除了代码之外与原书基本相同，[原书传送门](https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
